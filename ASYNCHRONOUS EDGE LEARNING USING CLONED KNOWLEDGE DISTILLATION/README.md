# ASYNCHRONOUS EDGE LEARNING USING CLONED KNOWLEDGE DISTILLATION 

## 背景

现有的联邦学习算法有着以下几个缺点：
* 当用户池（参与联邦学习的设备组成的集合）发生变化的时候，没办法迅速、有效率地处理这些变化
* 需要很大的传输带宽，并且当参与设备的数量增大的时候，需要的传输带宽也会线性地增大
* 需要所有参与某次联邦训练的设备将本地参数上传给中央服务器，这会产生网络延迟

## 论文贡献

这篇文章的主要贡献有以下三点：
* 将联邦学习问题重新表述为用户池动态变化的现实问题，称之为异步边缘学习(asynchronous edge learning)
* 提出了一种知识蒸馏(knowledge distillation, KD)的变种-克隆蒸馏(cloned distillation)，并解释什么时候、和为什么有效，也便是用这种方式代替了梯度上传
* 在利用克隆的知识蒸馏进行异步边缘学习的时候，提出一个可以有效地从分散的边缘数据集中学习并缓解核心模型（中央服务器上的模型）的灾难性遗忘问题的方法

## 异步边缘学习模型

在这里将用户设备称作`edge`，将中央服务器称做`core`，该模型与传统的联邦学习模型的几个区别是：
* 上传的并不是参数，而是经过知识蒸馏之后的一个参数，称作`Knowledge`
* 每个用户上的设备都是异步进行的

整体的模型结构如图所示：
![](images/1.png)

## 算法及训练流程

![](images/2.png)


