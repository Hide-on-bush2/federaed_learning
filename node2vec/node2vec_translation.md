# node2vec:网络的可拓展表征学习

## 摘要

在网络中，对于结点和边的预测任务需要用学习算法在工程特征上进行精心的工作。最近在表征学习的拓展领域的研究在自动预测上有了巨大的进步，其基本思想是学习特征本身。但是，当前的特征学习方法不足以表现出捕获网络中观察到的连接模式多样性的能力。  
  
在这里我们提出node2vec，一种可以学习网络中节点连续的特征表示的算法架构。在node2vec中，我们学习了节点到特征的低维空间的映射，该映射最大程度地保留了节点的网络邻域。我们定义了节点网络邻居的灵活概念，并设计了一种有偏的随机游走程序，该程序可以有效地探索各种邻居。我们基于之前对于网络领域的严格定义来总结先前的工作，并认为在探索领域的时候增加一些灵活性是学习到更丰富表示的关键。  
  
我们展示了，对于当前在多标签分类和在来自不同领域的几个实际网络中的链接预测，node2vec的效率。综上所述，我们的工作提出了一个的方法，可以在复杂网络中有效率地学习最先进的与任务无关的特征表示。  
  
**关键词**： 信息网络，表征学习，节点嵌入，图表示。  
  
## 介绍

很多在网络分析中的重要任务需要在节点和边上进行预测。在一个特定的节点分类任务中，我们感兴趣的是预测网络中节点最有可能具有的标签。举个例子，在社交网络中，我们也许关注的是预测用户的兴趣，或者在蛋白质相互作用网络中，我们关注与预测某个蛋白质的功能标签。与之类似，在链接预测中，我们希望能预测网络中的一对节点是否存在一条边将它们连接。链接预测在很多领域都有着广泛的应用。例如，在基因组学中，链接预测可以帮助我们找到基因之间新的相互作用，并且在社交网络中，它可以判别两个用户是否在现实世界中是好友。  
  
任何一种监督学习算法都需要一个信息化的，能够区分的和独立的特征集。在网络中的预测问题中，这意味着需要对节点和边构建一个特征的向量表示。一种典型的解决办法是根据专家的专业知识来手动设定特征。即便我们不考虑这种特征工程所需的繁琐工作，这种方法也是为特定任务而设计的，不能应用于不同的预测任务中。  
  
另一种方法是去通过解决一个最优化问题去学习特征表示。这种解决办法的难点在于定义一个目标函数，需要在计算效率和预测准确率中进行权衡。一方面，我们可以致力于寻找一种最优化预测任务效果下界的特征表示（只需要遍历训练集中的所有实例），这可以保证很高的预测准确度，但是需要很高的训练时间和花销。在另一个极端，可以将目标函数定义为预测任务下界的独立代表，并且可以以完全无监督的方式学习表示形式。这使得优化计算效率高，并经过精心设计的目标，从而产生了与任务无关的功能，这些功能与任务特定方法的预测准确性非常匹配。   
  
但是，当前的技术未能令人满意地定义和优化网络中可扩展的无监督特征学习所需的合理目标。基于线性和非线性降维技术的经典方法，例如主成分分析，多维标度及其扩展优化了转换网络代表数据矩阵的目标，这样就可以最大化数据表示的方差。因此，这些方法总是涉及适当数据矩阵的特征分解，这对于大型现实网络而言是昂贵的。而且，所得的潜在表示在网络上的各种预测任务上的性能较差。  
  
或者，我们可以设定一个目标：保存本地的节点领域。可以使用类似于单个隐层前馈神经网络上的反向传播的随机梯度下降（SGD）来有效地优化目标。最近在这个方向上的尝试[24，28]提出了有效的算法，但是依赖于网络邻域的严格概念，这导致这些方法对网络独有的连接模式不敏感。特别的，网络中的节点可以根据它们所属的社区进行组织（例如同性恋），除此之外，这些组织可以根据网络中节点的角色结构来分配（比如结构对等）。例如，在图1中，我们观察到节点u和s1属于同一紧密编织的节点社区，而两个不同社区中的节点u和s6共享中心节点的相同结构角色。现实世界的网络通常会表现出这些等效性的混合体。 因此，必须有一个灵活的算法，该算法能够遵循两个原则来学习节点表示形式：能够学习将来自同一网络社区的节点紧密地嵌入在一起的表示形式，以及学习共享相同角色的节点具有相似嵌入的表示形式 。 这将允许特征学习算法在广泛的领域和预测任务中进行概括。  
  
**目前的工作**。我们提出node2vec一个半监督的算法，用于网络的可拓展表征学习。我们使用由自然语言处理的先前工作激发的SGD优化基于图形的自定义目标函数。直观地，我们的方法返回了特征表示，这些特征表示可以最大程度地在d维特征空间中保留节点的网络邻居。我们使用二阶随机游走方法为节点生成（样本）网络邻域。  
  
我们主要的贡献是定义了一个节点邻域的灵活性概念。通过选择一个合适的邻域概念，node2vec可以学习一个根据节点的网络角色或者节点属于的网络社区所决定的表示。我们通过开发有偏差的随机游走族来实现这一目标，它可以有效率地探索一个给定节点的多样化邻域。这种算法是灵活的，我们可以通过可调参数老控制搜索的空间，这与之前工作的严格搜索方法不同。因此，我们的方法一般化了以前的工作，并且可以对网络中观察到的等效范围进行建模。支配我们搜索策略的参数具有直观的解释，并偏向于不同的网络探索策略。这些参数还可以通过半监督方式使用一小部分标记数据直接学习。  
  
我们也展示了单独的节点的特征表示是如何拓展到节点对（比如边）的。为了计算边的节点表示，我们使用简单的二进制运算符来组成各个节点的学习到的特征表示。这种组合性使node2vec可以进行涉及节点以及边缘的预测任务。  
  
我们的实验专注于两个普遍的网络预测任务：多标签分类任务，在这种任务中每一个节点都会具有一个或者多个标签；链接预测任务，在这种任务中我们预测两个节点之间是否会有着边的存在。我们将node2vec的性能与最新的特征学习算法进行对比。我们用来自不同领域的多个现实世界网络进行了实验，例如社交网络，信息网络以及系统生物学的网络。实验表明，node2vec在多标签分类上的性能比最新方法高26.7％，而在链接预测上的性能最高可达12.6％。该算法即使只有10％的标记数据也能显示出竞争性能，并且对噪声或边缘缺失形式的干扰也具有很好的鲁棒性。通过计算，node2vec的主要阶段可以轻松并行化，并且可以在几个小时内扩展到具有数百万个节点的大型网络。  
  
综上所述，我们的论文做出了一下贡献：

* 我们提出了node2vec，这是一种用于网络特征学习的有效可伸缩算法，该算法使用SGD有效地优化了一种新颖的网络感知的邻域保留目标。
* 我们展示了node2vec如何符合网络科学中既定的原理，为发现符合不同等价关系的表示形式提供了灵活性。
* 我们扩展了node2vec和其他基于特征的学习方法从节点到对的邻域保护目标基于边缘的预测任务的节点数量。
* 我们根据经验评估node2vec的多标签分类和几个实际数据集上的链接预测的实验效果。

本文的其余部分的结构如下。 在第2节中，我们简要概述了网络特征学习中的相关工作。 我们在第3节中介绍了使用node2vec进行特征学习的技术细节。在第4节中，我们根据经验评估了node2vec在各种现实网络中节点和边缘上的预测任务上的性能，并评估了参数敏感性，扰动分析和可扩展性方面。我们的算法。 我们以对node2vec框架的讨论作为结束，并在第5节中重点介绍一些将来的工作方向。在项目页面上可以找到node2vec的数据集和参考实现：[相关链接](http://snap.stanford.edu/node2vec。)

## 相关工作

机器学习领域在各种课题下对特征工程进行了广泛的研究。在网络课题中，用于生成节点特征的传统方法是基于特征提取技术，该技术通常包含一些基于网络属性的种子手工制作的特征。相反，我们的目标是通过将特征提取转换为表示学习问题来使整个过程自动化，在这种情况下，我们不需要任何手工设计的特征。  
  
无监督特征学习方法通​​常利用图形的各种矩阵表示的光谱特性，尤其是拉普拉斯矩阵和邻接矩阵。在这种线性代数观点下，这些方法可以看作是降维技术。提出了几种线性（例如PCA）和非线性（例如IsoMap）降维技术。这些方法都具有计算和统计性能方面的缺点。在计算效率方面，除非解决方案的质量因近似值而受到很大影响，否则数据矩阵的特征分解会很昂贵，因此，这些方法很难扩展到大型网络。其次，这些方法针对那些对于网络中观察到的各种模式（例如同质性和结构等效性）不稳健的目标进行了优化，并对基础网络结构与预测任务之间的关系做出了假设。例如，光谱聚类做出了强烈的同质假设，即图割对分类很有用。这样的假设在许多情况下是合理的，但不能有效地跨各种网络进行概括。用于自然语言处理的表示学习的最新进展为离散对象（例如单词）的特征学习开辟了新的途径。尤其是，Skip-gram模型旨在通过优化邻域保留似然目标来学习单词的连续特征表示。该算法的过程如下：它扫描文档中的单词，并针对每个单词嵌入该单词，以使单词的特征可以预测附近的单词（即，某些上下文窗口中的单词）。 通过使用带有负采样的SGD优化似然性目标来学习单词特征表示。Skip-gram目标基于分布假设，该假设指出相似上下文中的单词倾向于具有相似的含义。 即，相似的单词倾向于出现在相似的单词邻域中。  
  
受Skip-gram模型的启发，最近的研究通过将网络表示为“文档”来建立网络的比喻。与文档相同的方式是单词的有序序列，可以从基础网络中采样节点序列，然后将网络变成节点的有序序列。但是，节点有许多可能的采样策略，从而导致获得不同的学习特征表示。实际上，正如我们将要展示的那样，没有明确的获胜采样策略可在所有网络和所有预测任务中使用。这是先前工作的主要缺点，无法为从网络中采样节点提供任何灵活性。我们的算法node2vec通过设计一个不依赖于特定采样策略的灵活目标来克服了这一限制，并提供了参数来优化探索的搜索空间。  
  
最后，对于基于节点和边缘的预测任务，基于现有的和新颖的图特定的深层网络体系结构，有许多用于监督特征学习的最新工作。这些架构使用多层非线性变换直接将下游预测任务的损失函数最小化，这导致了高精度，但是由于高训练时间要求而以可扩展性为代价。  
  
## 特征学习框架

我们将网络中的特征学习化为最大似然优化问题。假定$G = (W, E)$是一个给定的网络。我们的分析是普遍的并且可以运用到所有的连接（非连接）和加权（无权）图中。假定$f:V->R^d$是在预测任务中需要学习到的节点到特征表示的一个特征映射。在这里$d$表示特征的数量（维度）。相同，$f$是一个大小为$|V| \times d$的矩阵。对于每一个源节点$u \in V$，我们用$N_S(u) \subset V$表示节点$u$的邻域。其中$S$下标表示抽样的策略。  
  
我们通过将Skip-gram架构扩展到网络来继续我们的工作。我们需要最优化以下目标函数，目的是最大化对于节点$u$观察到邻域为$N_S(u)$的对数概率：

$$max_f\sum_{u \in V}logP_r(N_S(u)|f(u))$$

为了使这个优化问题易于处理，我们做出两个基本的假设：

* 条件独立。简单来说：$Pr(N_S(u)|f(u)) = \prod_{n_i \in N_S(u)}Pr(n_i|f(u))$
* 特征空间是对成的。源节点和邻居节点在特征空间上对成。因此，我们将每个源邻节点对的条件似然建模为由其特征的点积参数化的softmax单位：

$$Pr(n_i|f(u)) = \frac{exp(f(n_i) * f(u))}{\sum_{v\in V}exp(f(v)*f(u))}$$

然后上诉的优化问题便可以转化为：

$$max_f\sum_{u\in V}[-logZ_u + \sum_{n_i \in N_S(u)f(n_i) * f(u)}]$$

但是$Z_u = \sum_{v\in V}exp(f(v)*f(u))$的计算代价太过昂贵，因此我们用随机梯度下降法来优化它。  
  
基于Skip-gram架构的特征学习方法最初是在自然语言的背景下开发的。给定文本的线性性质，可以使用连续单词上的滑动窗口自然定义邻域的概念。但是，网络不是线性的，因此需要更丰富的邻域概念。为了解决这个问题，我们提出了一个随机过程，对给定源节点u的许多不同邻域进行采样。邻域$N_S(u)$不仅限于直接邻居，还可以根据采样策略$S$具有极大不同的结构。













