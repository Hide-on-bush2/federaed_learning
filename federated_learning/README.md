#异构移动设备的统一联邦深度学习框架

## 摘要

移动设备会产生大量有价值数据，这为深度学习任务创造了条件。由于对于数据安全的考虑，将所有的数据记录到中心服务器去训练一个令人满意的模型是不实际的。联邦学习的应用使得参与的设备可以不上传它们的数据并在本地共同协作训练一个全局模型。但是用一个联合的方法来训练一个深度神经网络并不是一个简单的工作。这里面有三个主要的困难：资源受限，高传输成本和系统的异构性。为了克服这些困难，我们用卷积神经网络（最受欢迎的深度神经网络之一）作为例子提出一个统一的联合架构。首先，对于资源受限的问题，我们提出一个压缩CNN模型；其次，我们设计了一个可以最优化联邦传输效率的算法，来提高联邦传输效率；最后我们设计了三种训练的模式来融合在异构设备上训练的不同模型。我们在独立同分布的训练集和非独立同分布的训练集上组织了额外的实验，其结果证明了我们提出的架构可以在保持性能的基础上显著地减少通讯的成本。

## 关键词

* 联邦学习
* 异构网络
* 资源受限
* 通讯效率

## 1.介绍

智能移动设备，比如智能手机，平板电脑，可穿戴设备，自动交通工具等，变得越来越受欢迎。这些设备每天都会产生极大数量的有价值的数据，这些数据通常是受到保护的。但通过这些数据学习到的模型可以提高使用者的用户体验。举个例子，语言模型可以提高语音识别和文字输入的性能，图像模型可以提高它们自动选择好的图片的能力。现存的机器学习算法通常需要将这些数据全部上传到中央服务器来训练一个令人满意的模型。但是，由于用户数据收到保护，这种做法是不切实际的。最近，联邦学习被提出，这使得参与的设备可以不用上传它们在本地的数据，并与其他设备共同协作，在本地训练一个共享的全局模型。    

图片1展示了一个联邦学习应用于图像识别的例子。它如下进行工作：在每一轮通信中，移动设备连接到中央服务器并下载一个全局模型，并用本地的数据来训练这个模型，训练完毕之后再将更新传输到中央服务器。这个迭代过程一直持续到模型收敛或者遇到停止的条件。最后，每一个移动设备都会拥有一个全局的模型，并且可以将其运用在本地的图片识别任务中。并且，它在本地的表现将会跟在中央服务器用所有设备上传得到的数据一起训练得到的模型非常接近。    

联邦学习提供了可以保证数据安全性的全局训练方法，因此提高了用户的个人体验。由于（联邦学习）可以拥有更强大的计算能力和更大的移动设备存储空间，再加上对私人信息的担忧，联邦学习越来越具有吸引力。  
  
在移动设备上进行联邦学习并不是一件简单的工作，特别是对于拥有百万甚至十亿级别参数的深度学习网络来说。这里面主要有三个困难：

* 资源受限：一些移动设备由于有限的内存容量有着严格的限制。深度学习网络越来越大和复杂成为一个很严重的问题，并且大部分的硬件平台没办法跟上深度神经网络规模的指数级增长。
* 高传输成本：参与联邦学习的设备数量通常很多并且需要独立的带宽来上传更新。但是，移动设备拥有极少的上传带宽和昂贵的连接代价。
* 系统异构性：由于硬件的不一致（CPU，内存）和网络连接（3G，4G，Wi-Fi）,每一个设备在联邦学习上的存储空间，内存和连接质量都可能不同。此外，不同设备可能在同一时间运行不同的模型。

因此，对于异构的设备，如何设计一个统一的联邦深度学习框架是一个困难的问题。
卷积神经网络（CNN）作为最受欢迎的深度学习网络之一，在各个领域都有着显著的应用，包括图像识别，目标检测和语义分割。因此在本文，我们将用CNN作为一个例子来提出一个统一的联邦深度学习框架。   
  
由于上百上百万的参数，CNN的空间代价是承受不起的。比如说，一个8层的AlexNet有着超过600,000的参数，需要240MB的存储空间。在这种环境下，在资源受限的移动设备上展开CNN是一个严重的问题。与此相反的是，M.Deni et al. 展示了CNN网络是参数过载的，但只需要通过其中的一小部分参数便可以了解到属性。全连接层需要的内存消耗达到了所有内存消耗的89%甚至100%。因此，我们需要发现一个很直接的见解，来在保证整体表现的基础上降低参数的冗余。  
  
考虑到全连接层的权重矩阵是高度冗余的，我们使用一个它的low-rank表示来使其和资源有限的移动设备嵌合。参数的数量可以通过矩阵分解而减少很多。  
  
在全连接层的权重矩阵压缩之后，通信成本也在一定程度上得到降低。但是，移动设备具有极少的上传带宽或者昂贵连接代价。为了减少通信的代价，我们提出了一个新型通信高效通信的联合优化。这种算法与联邦平均算法有着不同之处，后者将每一个客户端的所有更新全部上传，前者指上传每个客户端的一部分更新。这样子不仅通信成本会降低，也可以避免梯度爆炸。  
  
对于异构系统来说，在联邦学习中的客户端设备可能会在同一个时间段跑不同的模型。但是，之前的一些研究只考虑了设备跑同一种模型的情况。在本论文，我们设计了三种训练模型来同时训练跑原始模型和压缩模型的异构移动设备。  
  
我们的贡献如下：
* 我们用一个全连接层权重矩阵的low-rank表示来使CNN模型与资源受限的移动设备嵌合。通过限制全连接层的权重矩阵的阶数，参数的数量可以被显著减少，并且整体表现不会收到显著降低。
* 基于压缩后的模型，我们提示了一个新型通信高效的联邦优化方法，通过只上传每个客户端的一部分更新到服务器，来降低通信成本和避免梯度爆炸。
* 与之前只考虑客户端设备跑同一种模型的研究不同，我们设计了三种训练模型来同时训练跑原始模型和压缩模型的异构移动设备。据我们所知，我们是第一个做这项工作的。

本文的剩余部分安排如下。我们在第二部分概述当前工作。在第三部分介绍我们的方法。在第四部分公布我们的实验结果。最后我们在第五部分总结我们的工作。

## 当前工作

在这部分，我们讨论当前与我们工作相关的研究。  
  
**通信效率**。在联邦学习中，移动设备必须频繁地对全局参数进行读写，这导致了巨大的通信成本。许多研究提出了许多解决分布式深度学习通信瓶颈的方法。一些研究致力于降低通信的次数。举个例子，不在每次迭代都进行通信，而是每个客户端在进行每两次次通信之间进行多次迭代。但是，这在客户端需要更复杂的计算来计算梯度。杨等。 另外一些研究提出了异步更新策略来执行局部模型的弹性平均。但是这些方法也许会降低模型的收敛速度。除此之外，矩阵量化和稀疏化（可以降低通信数据的规模）也被提出......这些提高通信效率的方法降低了通信效率，但是没有考虑移动设备资源有限的问题，这使得在这些设备上跑具有上百万规模参数的模型变得不现实。因此它们不适合应用在资源有限的移动设备上。  
  
**模型压缩**。一些移动设备在内存容量和存储方面有着严格的限制。在这些设备上进行深度学习网络模型的部署得到越来越多的关注。为了降低在资源有限的移动设备上部署的深度学习模型的规模，许多优化的技术被提出，比如向量量化，哈希技术，循环映射，参数剪枝和参数稀疏，这些方法可以极大减少模型的规模，并且效率不会显著下降。但是，这些模型需要对一些新操作进行额外的定义，因此很难将其运用在联邦学习上。除了这些方法，在很长的一段时间里人们使用low-rank方法来压缩深度学习模型，一些研究者运用奇异值分解来压缩全连接层的权重矩阵。另外一些研究者用张量分解来近似预训练得到的权重参数然后对其进行微调来补偿表现损失。上述low-rank方法仅考虑通过最小化其欧几里得距离来使压缩权重与预训练权重之间的参数近似。对于没有预训练权重的联合学习，此设置确实存在问题。一些研究者提出了一种低等级的结构化更新方法，该方法可从联邦学习中使用较少数量的变量参数化的受限空间中直接学习更新。但是，结构化更新方法需要在每个回合中为每个客户独立地重新生成预定义的模式，从而导致自适应能力和效率低下。 此外，矩阵模型限制的结构可能会导致偏差，从而导致精度损失。

## 方法

### 准备工作

我们定义一个训练集$X=[x_1, x_2, ..., x_n] \in R^{d\times n}$，$d$是特征向量的维度，$n$是训练集的大小。CNN中的第$l$个FC层的前向传播可以写作：  

$$a^l = f(z^l), where z^l = W^la^{l-1} + b^l$$

权重矩阵$W^l \in R^{M^l \times N^l}$, 偏置向量$b^l \in R^{M^l}$定义了转换。向量$z, a$表示转换前后的激活单元，$f(.)$表示转换函数，比如sigmoid，tanh和ReLU。  
  
因为$M^l$和$N^l$很大，内存消耗是不能承受的。所以要对$W^l$进行压缩。state-of-art low-rank分解方法只考虑近似预训练参数$W^l$和压缩参数$\hat{W^l}$（通过最小化它们的欧几里得距离）。但是，在联邦学习中，每个设备在本地通过自身的数据计算更新的梯度。在联邦学习中预训练并执行参数分解确实存在问题。

### 压缩CNN模型

FC层受困于参数冗余，在深度学习网络中就内存消耗方面是一个瓶颈。在这一部分，我们介绍一个可部署在资源有限的移动设备上的压缩CNN模型，它首先使用基于low-rank分解的SVD方法来替换原始的FC层权重矩阵，然后用随机梯度下降法来最优化参数。图2展示了原始CNN模型和压缩后的CNN模型之间的比较。

#### low-rank分解

我们首先运用SVD方法来替换$W^l$:$USV^T, where U \in R^{M^l\times r}, V\in R^{N^l \times r} and S \in R^{r\times r}$（S是一个对角矩阵，r是SVD-rank）。我们得到$W^l$的低阶分解表示$A^l{B^l}^T$，其中$A^l = US^{\frac{1}{2}}, B^l = VS^{\frac{1}{2}}$。

#### 前向传播

第l层的前向传播可以写作：

$$a^l = f(z^l), where z^l = A^l{B^l}^Ta^{l-1} + b^l$$

特别的，如果CNN模型有m个全连接层，那么最最后一层$a^m$的输出可以表示为：

$$a^m = f。(A^m \times {B^m}^T)。...。f。(A^1 \times {B^1}^T)x + b^m + ... + b^1$$

#### 向后传播

更多的注意应该放在模型的预测能力，而不是最小化$\hat{W^l}$和$W^l$的散度。因此我们尝试最小化$||y - a^m||_{F}^2$，其中$y$是训练集中正确的标签集。随后，我们使用随机梯度下降法来解决非凸优化问题，然后压缩模型通过反向传播在各层之间进一步共同优化。目标函数可以写成：

$$min_{l=1...m}J(A^l, {B^l}^T, b^l) = \frac{1}{2}||y-a^m||_F^2$$

目标函数的误差可以表示为：

$$\delta^l = \begin{cases}
\frac{\partial J}{\partial a^l} \bigodot f'(z^l), & l = m\\
(A^{l+1}{B^{l+1}}^T)^T\delta^{l+1}\bigodot f'(z^l), & otherwise
\end{cases}$$

其中$\bigodot$表示逐元素乘法。  
  
目标函数相对于所有参数的梯度可以计算为：

$$\frac{\partial J}{\partial A^l} = \delta^l(a^{l-1})^TB^l, \frac{\partial J}{\partial {B^l}^T} = A^{lT}\delta^l (a^{l-1})^T, \frac{\partial J}{\partial b^l} = \delta^l$$

上述计算出来的梯度将用于更新参数，在第k次迭代中，更新参数如下：

$$A_{k+1}^l = A_k^l + \alpha_k\frac{\partial J}{\partial A^l}, {B_{k+1}^l}^T = {B_k^l}^T + \alpha_k\frac{\partial J}{\partial {B^l}^T}, b_{k+1}^l = b_k^l + \alpha_k\frac{\partial J}{\partial b^l}$$

其中$a$是学习率，迭代直到模型收敛为止。  
  
我们在表1中总结了使用SVD的FC层和原始FC层的内存使用。当$r < min(M^l, N^l)$时，我们的方法会极大地减少FC层的参数数量。此外，压缩后的CNN模型有以下重要的优势：

* 易于实现。SVD作为矩阵分解中的标志性工具，很容易实现。而且分解后的FC层很容易插入一个DNN学习进程中。
* 易于微调。当FC层被压缩后，使用反向传播对网络进行微调很简单。

### 通信效率联邦优化

在联邦学习中通信是一个主要的瓶颈。一方面，联邦学习中的移动设备数目庞大并且频繁地对全局参数进行读写，这需要巨大数目的网络带宽。但是，服务器的网络带宽往往是有限制的并且被所有运行的设备所共享，这导致了巨大的通信过载。令一方面，移动设备通常上传带宽很狭窄，并且连接代价也比较昂贵。而且上传的速度也非常慢，并且所有设备都需要等待每个同步周期中最慢的一个。  
  
在FC层的权重矩阵被压缩之后，每个周期传输的信息大小从$M^lN^l$降到$(M^l + N^l)\times r$。但是，对于那些只有很窄带宽的移动设备来说，还要进一步改进。图3展示了，我们提出一个新型的通信高效联邦优化方法（CEFO）来降低通信代价。  
  
在第k次迭代中，一个独立的联邦学习周期由以下步骤组成：

* 在客户端集中选择模型。我们选择一个客户端集的子集（这个子集包含的客户端数量大于0），每一个从服务器中下载一个当前的全局模型。所有层的权重矩阵表示为$A_k, B^T_k, b_k$

$$A_k = [A_k^m, A_k^{m-1}, ..., A_k^1] \\
B_k^T = [{B_k^m}^T, {B_k^{m-1}}^T, ..., {B_k^1}^T] \\
b_k = [b_k^m, b_k^{m-1}, ..., b_k^1]$$  

* 本地梯度更新。每个在子集中的客户端根据本地数据中的一小部分计算权重矩阵$A_k, B^T_k, b_k$的梯度$G_{A^k}, G_{{B^T}}^k, G_{b^k}$

$$G_{A^k} = [g(A_k^m), g(A_k^{m-1}), ..., g(A_k^1)] \\
G_{{B^T}^k} = [g({B_k^m}^T), g({B_k^{m-1}}^T), ..., g({B_k^1)}^T] \\
G_{b^k} = [g(b_k^m), g(b_k^{m-1}), ..., g(b_k^1)]$$

我们做的工作与之前的一些工作不同，后者需要将所有的更新$G_{A^k}, G_{{B^T}^k}, G_{b^k}$都发送到服务端，我们只随机选择一半的用户端发送更新$G_{A^k},G_{b^k}$，而其余的一半发送$G_{{B^T}^k}, G_{b^k}$。通过这种方式不仅第l个FC层的通信代价可以减少到$(M^l + N^l) \times r/2$，并且可以避免梯度爆炸。因为服务端只有一部分的梯度更新。

* 联合平均。服务端收集选择的客户端的梯度更新$G_{A^k}, G_{{B^T}^k}, G_{b^k}$，并将其平均作为全局的更新。计算公式如下，其中$(i)$表示第i个客户端：

$$\overline{G_{A_k}} = \frac{1}{i}(G_{A_k}^{(1)} + G_{A_k}^{(2)}) + ... + G_{A_k}^{(i)}) \\
\overline{G_{B_k}^T} = \frac{1}{i}({G_{B_k}^T}^{(1)} + {G_{B_k}^T}^{(2)} + ... + {G_{B_k}^T}^{(i)}) \\
\overline{G_{b_k}} = \frac{1}{i}(G_{b_k}^{(1)} + G_{b_k}^{(2)}) + ... + G_{b_k}^{(i)})$$

然后使用梯度更新来计算阶跃向量，之后用这个向量来更新参数。

$$A_{k+1}^l = A_k^l + \alpha_k\overline{G_{A_k}^l} \\
{B_{k+1}^l}^T = {B_k^l}^T + \alpha_k\overline{{G_{A_k}^l}^T} \\
b_{k+1}^l = b_k^l + \alpha_k\overline{G_{b_k}^l}$$

### 训练模式

因为系统的差异，各个设备在联邦学习中的存储空间，内存和通信的能力也可能存在差异。一些移动设备有着足够的内存去训练复杂的深度学习模型，而其他的移动设备可能存在着严格的限制。更主要的是，存在不同设备在同一时间段运行不同模型的可能性。举个例子，我们假设初始条件下所有的移动设备运行原始的全局模型。当资源被其他程序占用的时候，有一些设备不能继续训练复杂的神经网络，所以它们用压缩的神经网络来替换复杂的神经网络。我们将运行复杂神经网络的设备称作大设备，将运行压缩后的神经网络的设备叫做小设备。我们基于我们的框架提出三种训练模式，分别是同步模式，预训练剪枝模式和选择模式，来将运行不同模型的异构设备统一起来。这三种模式介绍如下：

#### 同步模式

原始模型和压缩模式同步训练。在一个独立的联邦学习周期中，执行以下步骤：

* 本地训练：在每一次迭代中，大设备和小设备分别同时训练权重矩阵$W$和low-rank权重矩阵$A$和$B$，然后大设备将权重矩阵$W$发送到服务器，而小设备发送$A$和$B$到服务器。
* 服务器处理：服务器计算$W^+ = (A\times B + W) / 2$，然后分解$W^+$为$A+$和$B+$
* 更新权重矩阵。小设备从服务器下载$A^+$和$B^+$，而大设备从服务器下载$W^+$

这是一个在联邦学习中很直观的解决方式。但是对原始模型进行分解可能会损失一些信息，因此导致准确率大幅度下降。

#### 预训练剪枝模式

之前的一些工作通常使用矩阵分解来近似预训练权重矩阵中的参数，然后用剪枝来补偿表现上的损失。基于这个想法，我们提出了一个预训练剪枝的模式，过程如下：

* 在大设备中进行预训练：在每一次迭代中，每一个大设备计算它的更新后的权重矩阵$W$然后发送到服务器。服务器接收这些更新后，对其进行平均，然后在下一次迭代中将更新后的全局模型发送给这些大设备。在一定的循环之后，我们得到全局模型的权重矩阵$W^+$
* 服务器处理：服务器分解预训练得到的权重矩阵为$W^+$为低阶的矩阵$A^+$和$B^+$，然后将其发送到小设备上。
* 在小设备上进行剪枝：所有的小设备以联合的方式对权重矩阵进行剪枝得到全局模型的权重矩阵$A^+$和$B^+$
* 更新权重矩阵：服务器计算$W^+ = A^+ \times B^+$，然后大设备从服务器中得到$W^+$

#### 选择模式

在选择模式中，在每一次迭代中，大设备先训练一个全局的权重矩阵，然后每一个小设备根据上一个周期大设备训练得到的全局权重矩阵计算它们的更新，具体过程如下：

* 在大设备中进行本地训练：每一次迭代中，每一个大设备训练它们的权重矩阵$W$并将其发送到服务器。
* 服务器分解：服务器接收并平均这些权重矩阵，得到一个全局的权重矩阵$W^+$，然后将其分解为$A^+$和$B^+$
* 在小设备中进行本地训练：小设备从服务器中下载$A^+$和$B^+$，然后根据本地数据计算它们的梯度更新，然后发送到服务器。
* 服务器计算：服务器接收来自所有小设备的矩阵$A^+$和$B^+$，然后计算$W^+ = A^+ \times B^+$
* 更新权重矩阵：大设备在下一次迭代中从服务器中下载$W^+$

## 训练结果和分析











































